{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ №3\n",
    "\n",
    "Используем предобработанные в рамках 1-ого домашнего задания датасет combine_df_prepocessed.pkl. Используем столбец 'clean_tweet'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотеки и загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T02:20:56.627053Z",
     "start_time": "2020-09-05T02:20:54.956044Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import en_core_web_md\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T02:18:17.493807Z",
     "start_time": "2020-09-05T02:18:15.989178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>model love you take with you all the time in ur</td>\n",
       "      <td>[model, love, you, take, with, you, all, the, ...</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "      <td>[model, love, take, time, ur]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society now motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "      <td>[factsguid, societi, motiv]</td>\n",
       "      <td>[factsguid, societi, motiv]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "3   4    0.0  #model   i love u take with u all the time in ...   \n",
       "4   5    0.0             factsguide: society now    #motivation   \n",
       "\n",
       "                                         clean_tweet  \\\n",
       "0  when father is dysfunctional and is so selfish...   \n",
       "1  thanks for lyft credit cannot use cause they d...   \n",
       "2                                bihday your majesty   \n",
       "3    model love you take with you all the time in ur   \n",
       "4                  factsguide society now motivation   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, love, you, take, with, you, all, the, ...   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "\n",
       "                                tweet_token_filtered  \\\n",
       "0  [father, dysfunctional, selfish, drags, kids, ...   \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "2                                  [bihday, majesty]   \n",
       "3                      [model, love, take, time, ur]   \n",
       "4                  [factsguide, society, motivation]   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "2                                  [bihday, majesti]   \n",
       "3                      [model, love, take, time, ur]   \n",
       "4                        [factsguid, societi, motiv]   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...  \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...  \n",
       "2                                  [bihday, majesti]  \n",
       "3                      [model, love, take, time, ur]  \n",
       "4                        [factsguid, societi, motiv]  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('../lesson01/result.pkl.zip', compression='zip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T02:36:40.916406Z",
     "start_time": "2020-09-05T02:36:40.885413Z"
    }
   },
   "outputs": [],
   "source": [
    "train_clean_tweet = ' '.join(df.clean_tweet.values)[:1000000]\n",
    "test_clean_tweet = ' '.join(df[df['label'].isnull()].clean_tweet.values)[:1000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1.\n",
    "\n",
    "Используя библиотеку Spacy, вывести ТОП-20 популярных NER в combine_df датасете. Какой тип NER (ORG, GPE, PERSON и тд) оказался самым популярным? (Учтите, что max_word_limit_spacy для Spacy = 1000000)\n",
    "С помощью Spacy выяснить: какие персоны и организации самые обсуждаемые в train и test датасетах? вывести ТОП-20 самых популярных. \n",
    "\n",
    "Действительно ли в топ вошли только персоны и организации или есть мусор?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T02:36:48.802310Z",
     "start_time": "2020-09-05T02:36:41.858117Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T02:36:48.821420Z",
     "start_time": "2020-09-05T02:36:48.808730Z"
    }
   },
   "outputs": [],
   "source": [
    "def getNERCounters(text, nlp):\n",
    "    cp = Counter()\n",
    "    cn = Counter()\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ not in ['DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']:\n",
    "            cp[ent.label_] += 1\n",
    "        if ent.label_ in ['PERSON', 'ORG']:\n",
    "            cn[ent.text] += 1\n",
    "    return cp.most_common(3), cn.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T02:37:38.327483Z",
     "start_time": "2020-09-05T02:36:48.825421Z"
    }
   },
   "outputs": [],
   "source": [
    "train_counters = getNERCounters(train_clean_tweet, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T02:37:38.361959Z",
     "start_time": "2020-09-05T02:37:38.334885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "для TRAIN данных\n",
      "   TOP 3 типов:  [('PERSON', 3374), ('GPE', 1671), ('ORG', 1473)]\n",
      "   TOP 20 персон/организаций:  [('obama', 28), ('sta', 24), ('deletetweets', 20), ('suppo', 20), ('islam', 20), ('nba', 19), ('gop', 19), ('hillary', 18), ('sjw', 16), ('bing bong bing bong', 14), ('lebron', 11), ('bjp', 11), ('christina grimmie', 10), ('shi', 10), ('melancholy melancholymusic', 10), ('fed', 10), ('wso', 10), ('bihday', 10), ('cavs', 10), ('hu', 9)]\n"
     ]
    }
   ],
   "source": [
    "print('для TRAIN данных')\n",
    "print('   TOP 3 типов: ', train_counters[0])\n",
    "print('   TOP 20 персон/организаций: ', train_counters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T02:38:24.731492Z",
     "start_time": "2020-09-05T02:37:43.836071Z"
    }
   },
   "outputs": [],
   "source": [
    "test_counters = getNERCounters(test_clean_tweet, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T02:38:24.739303Z",
     "start_time": "2020-09-05T02:38:24.734835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "для TEST данных\n",
      "   TOP 3 типов:  [('PERSON', 3392), ('GPE', 1535), ('ORG', 1484)]\n",
      "   TOP 20 персон/организаций:  [('suppo', 28), ('obama', 26), ('sta', 21), ('gop', 20), ('ht', 16), ('nba', 16), ('deletetweets', 14), ('christina grimmie', 12), ('islam', 11), ('feminismisterrorism feminismmuktbharat', 11), ('sjw', 11), ('clinton', 10), ('wa', 10), ('bihday', 9), ('facebook', 9), ('hu', 9), ('donald trump', 9), ('lebron', 9), ('jo cox', 9), ('bong bing bong', 9)]\n"
     ]
    }
   ],
   "source": [
    "print('для TEST данных')\n",
    "print('   TOP 3 типов: ', test_counters[0])\n",
    "print('   TOP 20 персон/организаций: ', test_counters[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-04T23:43:43.410Z"
    },
    "scrolled": false
   },
   "source": [
    "> **Вывод**\n",
    "\n",
    "> * Для тестовых и обучаемых данных самый популярный тип PERSON \n",
    ">   * на первом месте в TRAIN - Omaba\n",
    ">   * на первом месте в TEST - Suppo - скорее всего мусор ? на втором месте Obama\n",
    "> * В результате есть мусор\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2.\n",
    "\n",
    "Используя библиотеку nltk, вывести ТОП-20 популярных NER в combine_df датасете. Какой тип NER (ORG, GPE, PERSON и тд) оказался самым популярным? Для данного задания используем ограничение на количество символов во входном датасете (max_word_limit_spacy = 1000000), чтобы иметь возможность сравнить результаты работы Spacy и nltk. Обратите внимание, что nltk чувствителен к регистру.\n",
    "С помощью nltk выяснить: какие персоны и организации самые обсуждаемые в train и test датасетах? вывести ТОП-20 самых популярных. \n",
    "\n",
    "Действительно ли в топ вошли только персоны и организации или есть мусор?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T03:32:14.119799Z",
     "start_time": "2020-09-05T03:32:14.085269Z"
    }
   },
   "outputs": [],
   "source": [
    "train_clean_tweet = ' '.join(df.tweet.values)[:1000000]\n",
    "test_clean_tweet = ' '.join(df[df['label'].isnull()].tweet.values)[:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T03:32:14.391052Z",
     "start_time": "2020-09-05T03:32:14.380952Z"
    }
   },
   "outputs": [],
   "source": [
    "def getNLTKCounters(text):\n",
    "    cp = Counter()\n",
    "    cn = Counter()\n",
    "    chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)))\n",
    "    for chunk in chunks:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            txt = ' '.join(c[0] for c in chunk)\n",
    "            label = chunk.label()\n",
    "            if label not in ['DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']:\n",
    "                cp[label] += 1\n",
    "            if ent.label_ in ['PERSON', 'ORGANIZATION']:\n",
    "                cn[txt] += 1\n",
    "    return cp.most_common(3), cn.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T03:34:50.093212Z",
     "start_time": "2020-09-05T03:32:14.596634Z"
    }
   },
   "outputs": [],
   "source": [
    "train_counters = getNLTKCounters(train_clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T03:34:50.102715Z",
     "start_time": "2020-09-05T03:34:50.096734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "для TRAIN данных\n",
      "   TOP 3 типов:  [('ORGANIZATION', 13), ('GPE', 5), ('PERSON', 1)]\n",
      "   TOP 20 персон/организаций:  [('u.s.', 3), ('en-ger-land', 1), ('nycÂ', 1), ('travelÂ', 1), ('mr. velicaria', 1), ('atÂ', 1), ('whiteaddictÂ', 1), ('inc.', 1), ('leastâ\\x80¦', 1), ('perÃº', 1), ('sÃ³ria', 1), ('____________________________________', 1), ('firstÂ', 1), ('moanaÂ', 1), ('ludik.hugo', 1), ('isrâ\\x80¦', 1), ('littleÂ', 1)]\n"
     ]
    }
   ],
   "source": [
    "print('для TRAIN данных')\n",
    "print('   TOP 3 типов: ', train_counters[0])\n",
    "print('   TOP 20 персон/организаций: ', train_counters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T03:36:30.070727Z",
     "start_time": "2020-09-05T03:34:50.105857Z"
    }
   },
   "outputs": [],
   "source": [
    "test_counters = getNLTKCounters(test_clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T03:36:30.093049Z",
     "start_time": "2020-09-05T03:36:30.073744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "для TEST данных\n",
      "   TOP 3 типов:  [('ORGANIZATION', 19), ('GPE', 9), ('PERSON', 2)]\n",
      "   TOP 20 персон/организаций:  [('u.s.', 7), ('fÃªte', 2), ('mr.', 2), ('whatÂ', 1), ('ó¾\\x93¯', 1), ('felicitÃ', 1), ('aldilÃ', 1), ('___', 1), ('yoursÂ', 1), ('peace/sad', 1), ('________________________________________', 1), ('newsÂ', 1), ('coolÂ', 1), ('liberal/left', 1), ('husbandÂ', 1), ('atÂ', 1), ('st.', 1), ('zÃ¼rich', 1), ('gÃ¼mbet', 1), ('u.s', 1)]\n"
     ]
    }
   ],
   "source": [
    "print('для TEST данных')\n",
    "print('   TOP 3 типов: ', test_counters[0])\n",
    "print('   TOP 20 персон/организаций: ', test_counters[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-04T23:43:43.410Z"
    },
    "scrolled": false
   },
   "source": [
    "> **Вывод**: нужно брать не очищенные данные и даже в этом случае много мусора потому что если делать lower вообще ничего не находит"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3.\n",
    "\n",
    "Какая из библиотек по вашему лучше отработала? Сравните качество полученных most_common NER и количество распознаных NER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Лучше всего отработала Spacy потому, для NLTK пришлось делать какой то хак потому что она не понимала lower() кроме того NLTK работает очень медлено и находит меньше NER а больше мусора**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
