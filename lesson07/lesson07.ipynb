{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ №7\n",
    "\n",
    "- Запустить seq2seq, seq2seq с внимаием и трансформер для перевода русских слов + описать наблюдения по качеству\n",
    "- Данные в папке data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:20:05.146431Z",
     "start_time": "2020-09-29T06:20:04.335500Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:16:00.868886Z",
     "start_time": "2020-09-29T06:16:00.864689Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 2\n",
    "latent_dim = 256\n",
    "num_samples = 10000\n",
    "data_path = 'data/rus-eng/rus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:16:03.362765Z",
     "start_time": "2020-09-29T06:16:02.020663Z"
    }
   },
   "outputs": [],
   "source": [
    "# Собираем из текстов токены и делаем pne-hot вектора на каждый токен\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:16:03.374623Z",
     "start_time": "2020-09-29T06:16:03.366145Z"
    }
   },
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:16:04.640445Z",
     "start_time": "2020-09-29T06:16:04.636157Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:16:06.553729Z",
     "start_time": "2020-09-29T06:16:05.571933Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:18:43.083418Z",
     "start_time": "2020-09-29T06:16:08.809015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "8000/8000 [==============================] - 71s 9ms/step - loss: 1.1050 - accuracy: 0.7735 - val_loss: 0.8977 - val_accuracy: 0.7595\n",
      "Epoch 2/2\n",
      "8000/8000 [==============================] - 69s 9ms/step - loss: 0.7195 - accuracy: 0.8074 - val_loss: 0.7621 - val_accuracy: 0.7938\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Do it.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Hurry!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Я                                                            \n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Shoot!\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: По                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Eat it.\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Eat up.\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: То                                                           \n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: По                                                           \n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: По                                                           \n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:19:54.207735Z",
     "start_time": "2020-09-29T06:19:53.142509Z"
    }
   },
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(preprocess_sentence(input_text))\n",
    "    target_texts.append(preprocess_sentence(target_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:19:55.179310Z",
     "start_time": "2020-09-29T06:19:55.174654Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:19:56.736839Z",
     "start_time": "2020-09-29T06:19:56.177635Z"
    }
   },
   "outputs": [],
   "source": [
    "input_tensor, inp_lang_tokenizer = tokenize(input_texts)\n",
    "target_tensor, targ_lang_tokenizer = tokenize(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:20:13.720346Z",
     "start_time": "2020-09-29T06:20:13.703613Z"
    }
   },
   "outputs": [],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:20:16.220619Z",
     "start_time": "2020-09-29T06:20:16.203886Z"
    }
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:20:19.923103Z",
     "start_time": "2020-09-29T06:20:19.901040Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.lstm(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "    \n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.lstm(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:20:21.941948Z",
     "start_time": "2020-09-29T06:20:21.918473Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:20:23.011275Z",
     "start_time": "2020-09-29T06:20:23.004991Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:30:32.284354Z",
     "start_time": "2020-09-29T06:25:09.343635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 1 Loss 0.0318\n",
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Epoch 2 Loss 0.0260\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        if not batch % 25: \n",
    "            print(f'Step {batch}')\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые украденные функции для оценки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:31:11.133503Z",
     "start_time": "2020-09-29T06:31:11.099866Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "\n",
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 14}\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()\n",
    "    \n",
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T06:31:14.213519Z",
     "start_time": "2020-09-29T06:31:12.554348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xander/anaconda3/lib/python3.7/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['char', 'f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> good morning <end>\n",
      "Predicted translation: ! <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xander/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "/Users/xander/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: FixedFormatter should only be used together with FixedLocator\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ticker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c62fe7b3b252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pylab'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'good morning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-61331b77bd12>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted translation: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_plot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mplot_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-61331b77bd12>\u001b[0m in \u001b[0;36mplot_attention\u001b[0;34m(attention, sentence, predicted_sentence)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfontdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_yticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpredicted_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfontdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_major_locator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultipleLocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_major_locator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultipleLocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ticker' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "translate(u'good morning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
